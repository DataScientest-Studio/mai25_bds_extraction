{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a84f7f-71ef-49d6-9993-2e18d6f016c6",
   "metadata": {},
   "source": [
    "# Modèles multimodaux - CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abf6b4-8fec-440c-a7f1-33ac254319e2",
   "metadata": {},
   "source": [
    "## README\n",
    "Ce notebook permet la création et l'évaluation d'un modèle basé sur l'architecture existatne CLIP.\n",
    "\n",
    "# TODO\n",
    "\n",
    "Le chapitre 1 prépare les données nécessaires\n",
    "\n",
    "Le chapitre 2 crée et évalue un modèle de voting simple, par moyennage des prédictions de 2 modèles (1 images et 1 texte)\n",
    "\n",
    "Le chapitre 3 propose une version avancée, avec pondérations, apprises ou non, des résultats des deux modèles utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44804c6-78ae-482f-84c6-f6a741e8bd85",
   "metadata": {},
   "source": [
    "## 1. Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9a69b-bef6-4d16-bd29-ed9c8f074f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec9d71-5eca-472d-84fe-924c40100acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from src.models.multimodal import MultiModalVoter, MultiModalClassWeightedVoter, MultiModalLogisticRegressor\n",
    "from src.visualization.visualize import visual_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8697f72-68e1-4ac8-a304-049643520506",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78566542-9924-4aac-9d25-a15d13d65327",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = pd.read_parquet(os.path.join(PATHS.processed_data, 'df_txt_ocr1.parquet'))\n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927af13-7788-4d88-b009-03eebc098b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans src/utils?\n",
    "def get_converted_image_path(tif_path):\n",
    "    return os.path.join(\n",
    "        PATHS.converted_images,\n",
    "        tif_path.replace('raw/RVL-CDIP/images/', '').replace('.tif', '.jpg')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5840f-12a5-4c7d-bd31-8e1191c5b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = pd.read_parquet(os.path.join(PATHS.metadata,'df_filepaths.parquet'))[['rvl_image_path']]\n",
    "image_features[\"filepath\"] = image_features.rvl_image_path.apply(get_converted_image_path)\n",
    "image_features.drop(columns=\"rvl_image_path\", inplace=True)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93acf6-dbd8-4c3d-b125-88d78e753be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = text_features.join(image_features, how=\"inner\")\n",
    "del image_features, text_features\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb4c85-57b6-40af-ab93-02210642d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = pd.read_parquet(os.path.join(PATHS.metadata, \"df_data_sets.parquet\"))\n",
    "labels = pd.read_parquet(os.path.join(PATHS.metadata, \"df_encoded_labels.parquet\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10849026-1c94-4832-888e-0057012b1132",
   "metadata": {},
   "source": [
    "## pour ne travailler que sur un échantillon :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6e66607-111c-4159-a20e-75c98ae66118",
   "metadata": {},
   "source": [
    "sample = pd.read_parquet(os.path.join(PATHS.metadata, 'samples', 'df_documents_sample_1k_3.parquet'))\n",
    "features = features.join(sample, how='inner')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481163a-41ba-4b54-84be-1d04a93cb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# au cas où features soit incomplet\n",
    "data_sets = data_sets.join(features[[]], how=\"inner\")\n",
    "labels = labels.join(features[[]], how=\"inner\")\n",
    "\n",
    "features.shape, data_sets.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67ebb5-d49d-43ff-9c8e-808807053786",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features[data_sets.data_set == \"train\"]\n",
    "y_train = labels[data_sets.data_set == \"train\"]\n",
    "\n",
    "X_val = features[data_sets.data_set == \"val\"]\n",
    "y_val = labels[data_sets.data_set == \"val\"]\n",
    "\n",
    "X_test = features[data_sets.data_set == \"test\"]\n",
    "y_test = labels[data_sets.data_set == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17388ba2-7aaf-41d3-a706-fbb05c4c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.multimodal_clip import MultimodalCLIPBasedClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b52ee-7d8f-480d-a0b1-44c9296bf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultimodalCLIPBasedClassifier()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9319e707-c9c8-425c-b8aa-c9bcc685a5db",
   "metadata": {},
   "source": [
    "t0 = time.time()\n",
    "embeddings = clf.get_clip_embeddings(features)\n",
    "print(f\"Terminé en {time.time()-t0:.2f} secondes\")\n",
    "embeddings.to_parquet(os.path.join(PATHS.processed_data, 'df_clip_embeddings.parquet'))\n",
    "# ENVIRON 8H10 DE CALCUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed037e1-e41a-40e6-9b35-8085d6d66e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_parquet(os.path.join(PATHS.processed_data, 'df_clip_embeddings.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c1edd-e9ba-4006-9db4-1046a7b76562",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02064da8-d27b-4f37-b77b-3722fb3e9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "clf.fit(X_train, y_train, embeddings=X_train[[]].join(embeddings))\n",
    "print(f\"Terminé en {time.time()-t0:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc10e47-6266-4565-94d5-8154b4529bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.visualize import visual_classification_report\n",
    "visual_classification_report(clf, X_test, y_test, \"CLIP-based Logistic Regressor Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0528904-f32f-4cf1-97bd-9c18737e9e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
