{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119e67c-6b6d-48ed-97f5-22cc64d5dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA #tester prince ? \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from skopt import BayesSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8f388-f9a4-4ba3-8570-c8cb9c9c9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS\n",
    "from src.visualization.visualize import plot_spider_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cfdb9-3d05-4ee2-84a4-749f4875264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_parquet(os.path.join(PATHS.processed_data,\"ML_images_100x100\", \"df_pixels_features.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703c459-c79a-408b-bf4f-2b191fab436d",
   "metadata": {},
   "source": [
    "## pour ne travailler que sur un échantillon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a75a1-cf0f-4445-bfc2-3eba8377bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_parquet(os.path.join(PATHS.data,'metadata', \"samples\", 'df_documents_sample_4k_3.parquet'), engine='fastparquet')\n",
    "sample = sample.drop(columns=['filename', 'rvl_image_path', 'label', 'data_set',\n",
    "       'iit_image_path', 'iit_individual_xml_path', 'iit_collective_xml_path'])\n",
    "df = pd.merge(df_all, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e71a0-27db-4e0a-a393-8697a0cbe664",
   "metadata": {},
   "source": [
    "## création des sets train, test et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33ce64-a403-413d-b212-ebbf0701f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les noms des colonnes sauf 'cat' et 'set'\n",
    "features = df.columns.difference(['cat', 'set'])\n",
    "\n",
    "# Split\n",
    "X_train = df[df['set'] == 'train'][features]\n",
    "X_val   = df[df['set'] == 'val'][features]\n",
    "X_test  = df[df['set'] == 'test'][features]\n",
    "\n",
    "y_train = df[df['set'] == 'train']['cat']\n",
    "y_val   = df[df['set'] == 'val']['cat']\n",
    "y_test  = df[df['set'] == 'test']['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741e775-3bf7-474c-bc9c-4f74fa1b000b",
   "metadata": {},
   "source": [
    "## normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac10db-66fb-49cc-9b6d-36ed571ce2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes à normaliser\n",
    "cols_to_normalize = ['top_marge', 'bottom_marge', 'left_marge',\n",
    "       'right_marge', 'nb_lignes', 'nb_colonnes', 'sharpness', 'noise',\n",
    "       'ratio_b', 'ratio_n', 'entropy','width']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit sur le train\n",
    "X_train[cols_to_normalize] = scaler.fit_transform(X_train[cols_to_normalize])\n",
    "\n",
    "# Transform val et test\n",
    "X_val[cols_to_normalize] = scaler.transform(X_val[cols_to_normalize])\n",
    "X_test[cols_to_normalize] = scaler.transform(X_test[cols_to_normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9e82b-6a37-4730-9400-c0d49a4888d7",
   "metadata": {},
   "source": [
    "## PCA pour garder 2000 colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29fa46-8006-48d4-9d65-a7e33362825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2000  \n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3289b-bb09-4b45-a92c-c9161b276c60",
   "metadata": {},
   "source": [
    "## LGBM avec les meilleurs paramètres\n",
    "\n",
    "Meilleurs hyperparamètres :\n",
    "- 'colsample_bytree', 0.5802168967298673\n",
    "- 'learning_rate', 0.029216387145600653\n",
    "- 'max_depth', 15\n",
    "- 'min_child_samples', 69\n",
    "- 'n_estimators', 500\n",
    "- 'num_leaves', 30\n",
    "- 'reg_alpha', 1.3883805031132697e-08\n",
    "- 'reg_lambda', 0.00016690235239007222\n",
    "- 'subsample', 0.654130102375878\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25b90b-7310-487f-80ef-13797956b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = LGBMClassifier(num_leaves=30, \n",
    "                     max_depth=15, \n",
    "                     learning_rate=0.03, \n",
    "                     n_estimators=500, \n",
    "                     subsample_for_bin=200000, \n",
    "                     min_child_samples=69, \n",
    "                     subsample=0.65, \n",
    "                     colsample_bytree=0.58, \n",
    "                     reg_alpha=0.0, \n",
    "                     reg_lambda=0.0, \n",
    "                     n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(\"\\n Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.show()\n",
    "\n",
    "#DataFrame des scores\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "print(\"\\n Rapport sous forme de DataFrame :\")\n",
    "print(report_df.head(16))  # Affiche les 16 classes\n",
    "print('')\n",
    "print('###############################')\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\" Temps d'exécution total : {elapsed / 60:.2f} minutes ({elapsed:.1f} secondes)\")\n",
    "\n",
    "#Remarque : le set de validation servirait à tester l'algo final qui aurait déjà vu X_train et X_test. Il ne sert pas ici. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
