{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d62f92-bf90-4417-bdbf-91ea85bd6356",
   "metadata": {},
   "source": [
    "# Extraction des données IIT-CDIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e9ffa-4686-4392-82a9-f5dc1bfea767",
   "metadata": {},
   "source": [
    "## README\n",
    "Ce notebook permet de télécharger sur le site \"https://data.nist.gov\" les images et métadonnées de la BDD IIT-CDIP associées aux images RVL-CDIP.\n",
    "\n",
    "Il réalise tout d'abord certaines opérations préalables (chapitre 1), dont la définition des variables globales d'exécution (**A METTRE A JOUR LORS D'UNE PREMIERE UTILISATION**)\n",
    "\n",
    "A l'issue (chapitre 2), il reconstitue la liste des identifiants de documents de la BDD RVL-CDIP qui se trouvent dans le répertoire *rvl_cdip_images_path*.\n",
    "\n",
    "Ensuite (chapitre 3), il identifie les images tar à télécharger (en ignorant celles qui ont déjà été pleinement exploitées), les télécharge sur le site *data.nist.gov* puis écrit sur le disque dur, dans l'arborescence du répertoire *iit_cdip_images_path*, les fichiers tif et xml concernés.\n",
    "\n",
    "Remarques:\n",
    "- Lors de cette phase, les fichiers .tar téléchargés sont volumineux (environ 2.5 GB par fichier, et environ 600 fichiers tar au total, ce qui représente plus d'1 To de données à télécharger). Toutefois, les fichiers à conserver ne réprésentent qu'un peu moins de 70Go. Pour éviter des écritures disque inutiles et l'occupation d'un très gros volume, il a été choisi de traiter les fichiers téléchargés à la volée, sans les écrire sur le disque dur. L'inconvénient est que si l'on souhaite accéder à nouveau à ce fichier tar, il faudra le télécharger à nouveau. L'avantage, au delà de l'économie d'usure des disques durs) est de limiter l'espace disque requis.\n",
    "- Dans les fichiers .tar téléchargés, certains documents ne disposent pas de fichiers xml associés. Dans ce cas, seule l'image est obtenue.\n",
    "\n",
    "Enfin (chapitre 4), le notebook permet de télécharger les fichiers xml \"collectifs\", dans lesquels sont rassemblées les métadonnées de plusieurs documents (environ 10000 par fichier). Les fichiers téléchargés sont traités à la volée pour être extraits et réduits afin de ne conserver que les métadonnées liées aux documents de la base de données RVL_CDIP. Ce traitement évite à nouveaux des accès disque et permet de faire passer le volume requis d'environ 24 Go à environ 3 Go.\n",
    "\n",
    "Remarque:\n",
    "- **[IMPORTANT] Au bilan, l'utilisation de ce script écrira environ 70 Go de fichiers tif et xml sur le disque dur et téléchargera environ 1,5 To d'internet**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062badf2-5141-40e7-afdb-1917f5c8c7e9",
   "metadata": {},
   "source": [
    "## 1. Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa7cef-8fd7-4a79-bb0b-af7c06077ee1",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f516d3-1792-4ef1-816c-2f50c5750386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gzip\n",
    "import time\n",
    "import requests\n",
    "import tarfile\n",
    "import platform\n",
    "import pandas as pd\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ed1e4-c855-43c8-9588-aa7df0a2545e",
   "metadata": {},
   "source": [
    "### 1.2 Variables globales\n",
    "Pourrait faire l'objet d ela mise en place de variables d'envirinnoment, ou d'un fichier de configuration local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaea39b-5e0c-420e-b1c5-e5c31a6b98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/Users/ben/Work/mle/ds-project/mai25_bds_extraction/' # à modifier par chacun en fonction de son arborescence\n",
    "\n",
    "data_path = os.path.join(project_path, 'data')\n",
    "raw_data_path = os.path.join(data_path, 'raw')\n",
    "processed_data_path = os.path.join(data_path, 'processed')\n",
    "# processed_data_path = os.path.join(data_path, 'processed')\n",
    "\n",
    "rvl_cdip_images_path = os.path.join(raw_data_path, 'RVL-CDIP', 'images')\n",
    "iit_cdip_images_path = os.path.join(raw_data_path, 'IIT-CDIP', 'images')\n",
    "iit_cdip_xmls_path = os.path.join(raw_data_path, 'IIT-CDIP', 'xmls')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfa03c-6171-4862-a98d-55e8b6b4810b",
   "metadata": {},
   "source": [
    "### 1.3 Nettoyage des fichiers \".DS_Store\" (utile sur Mac uniquement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e255271-17fb-4cc4-a3ec-ae270258c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ds_store_files(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Recursively removes all .DS_Store files from the specified directory and its subdirectories.\n",
    "    \n",
    "    This function traverses the given directory and all its subdirectories using os.walk().\n",
    "    It checks for the presence of any file named \".DS_Store\" and removes it. If an error occurs\n",
    "    during file removal, the exception is caught and an error message is displayed.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the root directory where the search for .DS_Store files will start.\n",
    "    \n",
    "    Returns:\n",
    "    None: This function does not return any value. It simply prints messages about the files removed\n",
    "          or any errors encountered during the process.\n",
    "    \n",
    "    Example:\n",
    "    remove_ds_store_files(\"/path/to/your/folder\")\n",
    "    \"\"\"\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \".DS_Store\":\n",
    "                file_path = os.path.join(folder, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Removed: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing {file_path}: {e}\")\n",
    "if platform.system() == 'Darwin':\n",
    "    remove_ds_store_files(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716c0b4-b564-4833-86a9-fa970547a801",
   "metadata": {},
   "source": [
    "## 2. Création de la liste des identifiants des images RVL-CDIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37328581-ea71-420b-a138-f975055f4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rvl_cdip_image_ids(rvl_cdip_images_path):\n",
    "    tmp_list = []\n",
    "    for foldername, _, filenames in os.walk(rvl_cdip_images_path):\n",
    "        if filenames:\n",
    "            filename = filenames[0]\n",
    "            # we check that the structure is relevant to our expectation with 2 asserts\n",
    "            assert len(filenames) == 1, f\"{foldername},{filename}\"\n",
    "            if filename.startswith('.'): # avoid to consider files like .DS_Store on mac\n",
    "                continue\n",
    "            assert filename.endswith(\".tif\"), f\"{foldername},{filename}\"\n",
    "            tmp_list.append((os.path.basename(foldername), filename))\n",
    "    tmp_list.sort()\n",
    "    return pd.DataFrame(tmp_list, columns = [\"document_id\", \"filename\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0461ca5-1aa8-4b0a-9a6a-ff850e3f98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "df = get_rvl_cdip_image_ids(rvl_cdip_images_path)\n",
    "print(f\"Duree d'exécution: {time.time() - t:.3f} secondes.\")\n",
    "print(f\"({len(df)} images traitées)\\n\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d7808-47fd-4e68-90b1-af56a95d48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"first_letter\"] = df.document_id.str.slice(stop = 1)\n",
    "df.loc[:, \"first_two_letters\"] = df.document_id.str.slice(stop = 2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddf6d6-9917-411a-943b-ed7148c4b9c7",
   "metadata": {},
   "source": [
    "## 3. Téléchargement des images et fichiers xml individuels de la base IIT-CDIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaffa5c-f89b-4681-87bb-bd6dcd04401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_images = df.groupby(\"first_two_letters\").agg(\n",
    "    {'document_id': list, 'filename': list}).rename(\n",
    "    columns={\"document_id\": \"document_ids\", \"filename\":\"filenames\"})\n",
    "groupby_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34c466-d7d2-41be-ae4f-55d7dd48175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_existing_ids(ids_list: list, filenames_list: list, iit_cdip_images_path: str, also_check_xmls = True):\n",
    "    \"\"\"From given lists of image ids and filenames, remove those for which image already exists in iit_cdip_images_path\n",
    "    Return a new list containing ids that do not exist yet.\n",
    "    \"\"\"\n",
    "    missing_ids = []\n",
    "    missing_filenames = []\n",
    "    for id_, filename in zip(ids_list, filenames_list):\n",
    "        expected_file_path = os.path.join(iit_cdip_images_path, f\"images{id_[0]}\", id_[0], id_[1], id_[2], id_, filename)\n",
    "        # expected_file_path = \"/Users/ben/Work/mle/ds-project/mai25_bds_extraction/data_sample/raw/IIT-CDIP/images/imagesa/a/a/a/aaa0a000/92464841_4842.tif\"\n",
    "        image_exists = os.path.exists(expected_file_path)\n",
    "        \n",
    "        if also_check_xmls:\n",
    "            expected_file_path = os.path.join(iit_cdip_images_path, f\"images{id_[0]}\", id_[0], id_[1], id_[2], id_, f\"{id_}.xml\")\n",
    "            xml_exists = os.path.exists(expected_file_path)\n",
    "        else:\n",
    "            xml_exists = True\n",
    "        if not (image_exists and xml_exists):\n",
    "            missing_ids.append(id_)\n",
    "            missing_filenames.append(filename)\n",
    "    return missing_ids, missing_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb9746-47a3-45ef-9922-a51ae1d87702",
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_images_base_url = \"https://data.nist.gov/od/ds/ark:/88434/mds2-2531/cdip-images/\"\n",
    "def import_iit_cdip_images_set(letters, ids, filenames, iit_cdip_images_path, iit_cdip_xmls_path, nist_images_base_url):\n",
    "    missing_files = [\n",
    "        os.path.join(f\"images{id_[0]}\", id_[0], id_[1], id_[2], id_, filename) \n",
    "                    for id_, filename in zip(ids, filenames)] + [\n",
    "        os.path.join(f\"images{id_[0]}\", id_[0], id_[1], id_[2], id_, f\"{id_}.xml\") \n",
    "                    for id_ in ids]\n",
    "    os.makedirs(iit_cdip_images_path, exist_ok=True)\n",
    "    # exemple d'url d'un dossier d'images sur NIST: \n",
    "    # https://data.nist.gov/od/ds/ark:/88434/mds2-2531/cdip-images/images.a.a.tar\n",
    "    url = f\"{nist_images_base_url}images.{letters[0]}.{letters[1]}.tar\"\n",
    "    print(\"   ... fetching \", url)\n",
    "    t = time.time()\n",
    "    tif_amount, xml_amount = 0, 0\n",
    "    reponse = requests.get(url, stream=True)\n",
    "    reponse.raise_for_status()  # En cas d'erreur HTTP\n",
    "    with tarfile.open(fileobj=io.BytesIO(reponse.content), mode=\"r|*\") as archive:\n",
    "        for file in archive:\n",
    "            if file.name in missing_files:\n",
    "                archive.extract(file, path=iit_cdip_images_path)\n",
    "                if file.name.endswith('.tif'):\n",
    "                    tif_amount += 1\n",
    "                else:\n",
    "                    xml_amount += 1\n",
    "    print(f\"   ... Successfully extracted {tif_amount} tif + {xml_amount} xml files, in {time.time() - t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e05555-4c0b-4664-86c6-4ee50b24501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, serie in groupby_images.iterrows():\n",
    "    letters = index\n",
    "    ids_list = serie.document_ids\n",
    "    filenames_list = serie.filenames\n",
    "    missing_ids, missing_filenames = remove_existing_ids(ids_list, filenames_list, iit_cdip_images_path, also_check_xmls = False)\n",
    "    print(f\"Processing letters {letters}: {len(missing_ids)} images to retrieve\")\n",
    "    if missing_ids:\n",
    "        import_iit_cdip_images_set(letters, missing_ids, missing_filenames, iit_cdip_images_path, iit_cdip_xmls_path, nist_images_base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0835d1-db49-47e8-9e7a-bbd8a7c6abb8",
   "metadata": {},
   "source": [
    "## 4. Téléchargement des xmls communs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca4820-9a59-413a-933e-63c83df4a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls de téléchargement=\n",
    "# https://data.nist.gov/od/ds/ark:/88434/mds2-2531/cdip-text/cdip-1.tar\n",
    "# de 1 à 6\n",
    "\n",
    "for n in range(1, 7):\n",
    "    print(f\"Downloading  du fichier {n}/6\")\n",
    "    url = f\"https://data.nist.gov/od/ds/ark:/88434/mds2-2531/cdip-text/cdip-{n}.tar\"\n",
    "    reponse = requests.get(url, stream=True)\n",
    "    with tarfile.open(fileobj=io.BytesIO(reponse.content), mode=\"r|*\") as archive:\n",
    "        for gz_info in archive:\n",
    "            letters = gz_info.name[8] + gz_info.name[10]\n",
    "            rvl_ids = rvl_cdip_df[rvl_cdip_df.document_id.str.startswith(letters)].document_id.tolist()\n",
    "            output_filename = os.path.join(iit_cdip_xmls_path, f\"{letters}.xml\")\n",
    "            z_file = archive.extractfile(gz_info)\n",
    "            compressed_data = z_file.read()\n",
    "            decompressed_data = gzip.decompress(compressed_data)\n",
    "            try:\n",
    "                xml_file = io.BytesIO(decompressed_data)\n",
    "                parser = etree.XMLParser(recover=True)\n",
    "                tree = etree.parse(xml_file, parser)\n",
    "                root = tree.getroot()\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur de parsing XML : {e}\")\n",
    "                continue    \n",
    "            print(gz_info.name, letters, \":\", \"existing records=\", len(root.findall(\"record\")), end=\"\")\n",
    "            for record in root.findall(\"record\"):\n",
    "                docid_text = getattr(record.find(\"docid\"), \"text\", None)\n",
    "                if docid_text not in rvl_ids:\n",
    "                    root.remove(record)\n",
    "            tree.write(output_filename)\n",
    "            print(\" kept \", len(root.findall(\"record\")), \" out of \", len(rvl_ids), \" possibles.\")\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
