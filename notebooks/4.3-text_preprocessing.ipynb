{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab906a1-4af0-4b1f-8224-6ed8b5aa1bbb",
   "metadata": {},
   "source": [
    "# Preprocessing text-mining\n",
    "Ce notebook a pour objectif le traitement des textes obtenus par océrisation des images.\n",
    "L'objectif est donc d'obtenir, pour chaque image (=document) un texte amélioré, permettant d'alimenter des modèles de Machine learning.\n",
    "\n",
    "La base de texte choisie est l'ocr collectif: comme cela a été vu procédément, il contient le plus grand nombre de données (environ 398k documents sur les 400k possibles) et les informations contenues semblent identiques à celles des documents individuels).\n",
    "\n",
    "Dans ce notebook, on va donc créer un dataframe qui aura ce format (les colonnes ocr_x sont des colonnes obtenues après traitement de raw_ocr):\n",
    "\n",
    "| document_id (index) | raw_ocr | ocr_1 |...| ocr_n | label \n",
    "\n",
    "En fin de notebook, il sera ainsi possible de créer les Dataframes X (ocr_x) et y (label) qui permettront d'alimenter les modèles.\n",
    "\n",
    "La gestion du test_train_split est faite en début de notebook, en respectant la répartition définie dans le dataset RVL-CDIP.\n",
    "\n",
    "Remarque: des contraintes temporelles (temps alloué au projet + temps de calculs requis) nous ont contraints à nous limiter à un seul jeu de données traitées (\"ocr_1\"). Nous avons toutefois laissé la structure imaginée pour montrer la démarche dans laquelle nous nous inscrivions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf9200-a72a-45d5-8f40-1ff3c065efb5",
   "metadata": {},
   "source": [
    "## 1. Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c09c12-24e0-4837-948c-0af4db139811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb46a27-3bcd-48f1-89cb-f87de0c2cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import jamspell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a30fb4-ca8f-45ca-8475-328ecf6d3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATHS.processed_data / \"df_raw_ocr.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8fbd4-2b24-4ca2-ba16-0b3d3c0439f3",
   "metadata": {},
   "source": [
    "# 2. Application des traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ab37c-64b1-434a-964d-c09079a31e1b",
   "metadata": {},
   "source": [
    "## 3.1. Création ocr_1\n",
    "Ce premier pipeline de traitement permettra d'arriver à la création de la colonne ocr_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3a2d9-2433-49ae-be62-2377754508d2",
   "metadata": {},
   "source": [
    "### 3.1.1. Suppression des pagesNbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33013d-623c-4e22-9e7c-d54abf3cc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a460ba-6bde-4019-a63e-346c1f6d7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_regex = re.compile(r'pgNbr=[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33397fb-d037-436c-bb60-03dc689d5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pgNbr(text):\n",
    "    return len(pg_regex.findall(text))\n",
    "# comptabilisation du nombre de pgNbr:\n",
    "\n",
    "def remove_pgNbr(text):\n",
    "    text = pg_regex.sub('', text)\n",
    "    return text\n",
    "\n",
    "avant = df.raw_ocr.apply(count_pgNbr).sum()\n",
    "df[\"ocr_tmp\"] = df.raw_ocr.apply(remove_pgNbr)\n",
    "apres = df.ocr_tmp.apply(count_pgNbr).sum()\n",
    "print(avant, apres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff579-ad2b-4193-aa79-74f2e665430a",
   "metadata": {},
   "source": [
    "### 3.1.2. Déséchappement html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af204190-df8e-43f1-9dbd-9885fb6429de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "def unescape_html(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return html.unescape(text)\n",
    "\n",
    "avant = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "df.loc[:,\"ocr_tmp\"] = df.ocr_tmp.apply(unescape_html)\n",
    "apres = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "avant, apres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620e812-590e-46d6-8037-e77d1d7e35fe",
   "metadata": {},
   "source": [
    "### 3.1.3. Correction OCR\n",
    "Nous allons pour cela utiliser l'outil jamspell\n",
    "\n",
    "#### <span style=\"color:red\">Cette partie est fonctionnelle mais a été désactivée pour des raisons de performance</span>\n",
    "(3' sur un échantillon de 1000 lignes ==> environ 20 heures, si la croissance est linéaire)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476457f6-4e23-497f-905a-cb232b1e0395",
   "metadata": {},
   "source": [
    "# Le but ici est d'utiliser une librairie de correction de texte océrisé nommée jamspell\n",
    "# L'emploi de cette bibliothèque peut se faire soit avant, soit après les traitements réalisés au 1.\n",
    "# Nous utiliserons les 2 approches, pour réaliser 2 colonnes:\n",
    "# - cleaned_ocr_jamspell_first \n",
    "# - cleaned_ocr_jamspell_last"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e7b0f10-faf4-414a-9f48-c790536aa92d",
   "metadata": {},
   "source": [
    "jamspell_model_path = PATHS.models / 'jamspell' / 'en.bin'\n",
    "assert jamspell_model_path.exists()\n",
    "jamspell_model_path = str(jamspell_model_path)\n",
    "corrector = jamspell.TSpellCorrector()\n",
    "corrector.LoadLangModel(jamspell_model_path)\n",
    "def apply_jamspell(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return corrector.FixFragment(text)\n",
    "\n",
    "t0 = time.time()\n",
    "df.loc[:,\"ocr_tmp\"] = df.ocr_tmp.apply(apply_jamspell)\n",
    "print(f\"{len(df)} lignes traitées en {time.time()-t0:.0f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981717a-be2b-4dfa-bb63-d2173ffe5e2c",
   "metadata": {},
   "source": [
    "### 3.1.4 Suppression des caractères spéciaux et des séquences ne correspondant pas à des informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de2f43-dd08-4055-a293-f689248d7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A améliorer pour prendre en compte les lignes\n",
    "# sans doute trop brutal / il faudra le réviser sur d'autres versions ultérieures?\n",
    "def basic_word_filter(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    text = text.lower()\n",
    "    # Attention, c'est brutal, ca supprime tous les chiffres aussi...\n",
    "    word_regex = re.compile(r'[a-z]{2,}')\n",
    "    text = ' '.join(word_regex.findall(text))\n",
    "    \n",
    "    return text\n",
    "df[\"ocr_tmp\"] = df.ocr_tmp.apply(basic_word_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521259a-fe15-46a5-ada0-45d8b9460a5f",
   "metadata": {},
   "source": [
    "### 3.1.5. Filtrage des stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f425a-631f-4ed5-b503-98c4d3b08239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "nltk.download('punkt_tab')\n",
    "# Télécharger les ressources NLTK nécessaires\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1643-3b93-4e99-9f13-8c1ce68c08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste des stop words en anglais\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour nettoyer une phrase\n",
    "def remove_stopwords(text):\n",
    "    if pd.isnull(text):  # gestion des valeurs manquantes\n",
    "        return \"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# Application sur la colonne raw_ocr\n",
    "df['raw_ocr_clean'] = df['ocr_tmp'].apply(remove_stopwords)\n",
    "\n",
    "# Affichage\n",
    "print(df[['ocr_tmp', 'raw_ocr_clean']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81614e-38a5-40c1-af3b-8bc3e68c576f",
   "metadata": {},
   "source": [
    "### 3.1.6. Application du Pipeline et création d'ocr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687daff-1470-492a-ae85-72212e68b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca27dd-3459-4df3-b966-468114cf38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16e596-b294-4985-b813-192bd8f8011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.models.preprocessing import text_preprocessing1\n",
    "from p_tqdm import p_map\n",
    "\n",
    "df = pd.read_parquet(PATHS.processed_data / \"df_raw_ocr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19052d77-6797-4a22-ba60-b92c26d420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version monothread / tres lente\n",
    "ocr1 = text_preprocessing1(df.raw_ocr, with_tqdm=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce6d85e-f202-43da-b9f7-8b30db990d88",
   "metadata": {},
   "source": [
    "# Version multithread / plus rapide (environ 3 heures) mais prise en charge du tqdm plus aléatoire\n",
    "texts = df['raw_ocr'].tolist()\n",
    "ocr1 = p_map(lambda x: text_preprocessing1(x), texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b99d1-9c9d-450a-976e-a67d7c1d8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocr1 = pd.DataFrame(ocr1, columns=[\"ocr\"], index = df.index)\n",
    "df_ocr1.to_parquet(PATHS.processed_data / \"df_txt_ocr1.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
