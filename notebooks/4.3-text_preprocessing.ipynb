{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab906a1-4af0-4b1f-8224-6ed8b5aa1bbb",
   "metadata": {},
   "source": [
    "# Preprocessing text-mining\n",
    "Ce notebook a pour objectif le traitement des textes obtenus par océrisation des images.\n",
    "L'objectif est donc d'obtenir, pour chaque image (=document) un texte amélioré, permettant d'alimenter des modèles de Machine learning.\n",
    "\n",
    "La base de texte choisie est l'ocr collectif: comme cela a été vu procédément, il contient le plus grand nombre de données (environ 398k documents sur les 400k possibles) et les informations contenues semblent identiques à celles des documents individuels).\n",
    "\n",
    "Dans ce notebook, on va donc créer un dataframe qui aura ce format (les colonnes ocr_x sont des colonnes obtenues après traitement de raw_ocr):\n",
    "\n",
    "| document_id (index) | raw_ocr | ocr_1 |...| ocr_n | label \n",
    "\n",
    "En fin de notebook, il sera ainsi possible de créer les Dataframes X (ocr_x) et y (label) qui permettront d'alimenter les modèles.\n",
    "\n",
    "La gestion du test_train_split est faite en début de notebook, en respectant la répartition définie dans le dataset RVL-CDIP.\n",
    "\n",
    "Remarque: des contraintes temporelles (temps alloué au projet + temps de calculs requis) nous ont contraints à nous limiter à un seul jeu de données traitées (\"ocr_1\"). Nous avons toutefois laissé la structure imaginée pour montrer la démarche dans laquelle nous nous inscrivions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf9200-a72a-45d5-8f40-1ff3c065efb5",
   "metadata": {},
   "source": [
    "## 1. Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c09c12-24e0-4837-948c-0af4db139811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb46a27-3bcd-48f1-89cb-f87de0c2cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a30fb4-ca8f-45ca-8475-328ecf6d3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATHS.processed_data / \"df_raw_ocr.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8fbd4-2b24-4ca2-ba16-0b3d3c0439f3",
   "metadata": {},
   "source": [
    "# 2. Application des traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ab37c-64b1-434a-964d-c09079a31e1b",
   "metadata": {},
   "source": [
    "## 3.1. Création ocr_1\n",
    "Ce premier pipeline de traitement permettra d'arriver à la création de la colonne ocr_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3a2d9-2433-49ae-be62-2377754508d2",
   "metadata": {},
   "source": [
    "### 3.1.1. Suppression des pagesNbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a460ba-6bde-4019-a63e-346c1f6d7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_regex = re.compile(r'pgNbr=[0-9]+')\n",
    "type(pg_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca51ccd-1d92-4d7e-84d1-7579e1bd598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33397fb-d037-436c-bb60-03dc689d5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pgNbr(text):\n",
    "    return len(pg_regex.findall(text))\n",
    "# comptabilisation du nombre de pgNbr:\n",
    "\n",
    "def remove_pgNbr(text):\n",
    "    text = pg_regex.sub('', text)\n",
    "    return text\n",
    "\n",
    "avant = df.raw_ocr.apply(count_pgNbr).sum()\n",
    "df[\"ocr_tmp\"] = df.raw_ocr.apply(remove_pgNbr)\n",
    "apres = df.ocr_tmp.apply(count_pgNbr).sum()\n",
    "print(avant, apres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff579-ad2b-4193-aa79-74f2e665430a",
   "metadata": {},
   "source": [
    "### 3.1.2. Déséchappement html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af204190-df8e-43f1-9dbd-9885fb6429de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "def unescape_html(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return html.unescape(text)\n",
    "\n",
    "avant = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "df.loc[:,\"ocr_tmp\"] = df.ocr_tmp.apply(unescape_html)\n",
    "apres = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "avant, apres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620e812-590e-46d6-8037-e77d1d7e35fe",
   "metadata": {},
   "source": [
    "### 3.1.3. Correction OCR\n",
    "Nous allons pour cela utiliser l'outil jamspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819ff4e-388d-4f50-88ab-63011cade675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le but ici est d'utiliser une librairie de correction de texte océrisé nommée jamspell\n",
    "# L'emploi de cette bibliothèque peut se faire soit avant, soit après les traitements réalisés au 1.\n",
    "# Nous utiliserons les 2 approches, pour réaliser 2 colonnes:\n",
    "# - cleaned_ocr_jamspell_first \n",
    "# - cleaned_ocr_jamspell_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce02a18-6d2f-438a-a20c-e27164a6eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jamspell\n",
    "jamspell_model_path = os.path.join(project_path, 'models', 'jamspell', 'en.bin')\n",
    "corrector = jamspell.TSpellCorrector()\n",
    "corrector.LoadLangModel(jamspell_model_path)\n",
    "def apply_jamspell(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return corrector.FixFragment(text)\n",
    "# avant = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "df.loc[:,\"ocr_tmp\"] = df.ocr_tmp.apply(apply_jamspell)\n",
    "# apres = df.ocr_tmp.str.contains(\"&lt;\").sum()\n",
    "# avant, apres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16fa62-7d55-4448-a383-7622c8bc991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981717a-be2b-4dfa-bb63-d2173ffe5e2c",
   "metadata": {},
   "source": [
    "### 3.1.4 Suppression des caractères spéciaux et des séquences ne correspondant pas à des informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de2f43-dd08-4055-a293-f689248d7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A améliorer pour prendre en compte les lignes\n",
    "# sans doute trop brutal / il faudra le réviser sur d'autres versions ultérieures?\n",
    "def basic_word_filter(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    text = text.lower()\n",
    "    # Attention, c'est brutal, ca supprime tous les chiffres aussi...\n",
    "    word_regex = re.compile(r'[a-z]{2,}')\n",
    "    text = ' '.join(word_regex.findall(text))\n",
    "    \n",
    "    return text\n",
    "df[\"ocr_tmp\"] = df.ocr_tmp.apply(basic_word_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521259a-fe15-46a5-ada0-45d8b9460a5f",
   "metadata": {},
   "source": [
    "### 3.1.5. Filtrage des stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f425a-631f-4ed5-b503-98c4d3b08239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "nltk.download('punkt_tab')\n",
    "# Télécharger les ressources NLTK nécessaires\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8f50b53-26bf-4e80-beef-ecbf66a97f98",
   "metadata": {},
   "source": [
    "### 3.1.5 Suppression des lignes ne contenant pas assez d'information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1643-3b93-4e99-9f13-8c1ce68c08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste des stop words en anglais\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour nettoyer une phrase\n",
    "def remove_stopwords(text):\n",
    "    if pd.isnull(text):  # gestion des valeurs manquantes\n",
    "        return \"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# Application sur la colonne raw_ocr\n",
    "df['raw_ocr_clean'] = df['ocr_tmp'].apply(remove_stopwords)\n",
    "\n",
    "# Affichage\n",
    "print(df[['ocr_tmp', 'raw_ocr_clean']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da8547-71e0-499c-b407-fbdf4ba63ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a0969-8fdc-4a5d-ad24-494e2ebdc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour affichage dans le notebook (spécifique Jupyter sur macOS)\n",
    "%matplotlib inline\n",
    "\n",
    "# Préparer le texte (limité à 500 000 caractères pour éviter les crashs)\n",
    "text = \" \".join(df['ocr_tmp'].dropna().astype(str).tolist())[:500000]\n",
    "\n",
    "# Générer le nuage sans stopwords\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nuage de mots (sans suppression)\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3802543-8e77-46a0-a44e-fb0dcb28fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81614e-38a5-40c1-af3b-8bc3e68c576f",
   "metadata": {},
   "source": [
    "### 3.1.6. Application du Pipeline et création d'ocr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16e596-b294-4985-b813-192bd8f8011b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00ad06-0849-4438-993b-9eec33ce5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "def clean_ocr_1(text):\n",
    "    text = remove_page_number(text)\n",
    "    text = unescape_html(text)\n",
    "    text = apply_jamspell(text)\n",
    "    text = basic_word_filter(text)\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # ...\n",
    "    return text\n",
    "if \"ocr_1\" in train.columns:\n",
    "    train.drop(columns=\"ocr_1\", inplace=True)\n",
    "train.insert(\n",
    "    loc=1,\n",
    "    column=\"ocr_1\",\n",
    "    value=train[\"raw_ocr\"].apply(clean_ocr_1),\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b37214-aed9-4129-a0fc-f0704f53688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Séparer les jeux\n",
    "train_df = df[df['data_set'] == 'train']\n",
    "val_df   = df[df['data_set'] == 'val']\n",
    "test_df  = df[df['data_set'] == 'test']  # facultatif pour l’instant\n",
    "\n",
    "# 3. Features et labels\n",
    "X_train = train_df['ocr_tmp']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_val = val_df['ocr_tmp']\n",
    "y_val = val_df['label']\n",
    "\n",
    "# 4. Vectorisation TF-IDF sur le train uniquement\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_val_vect   = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82196ff1-9d8b-4760-a60a-25a08fe5c124",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Modèles à tester\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4cd81-60a0-43dd-ad14-e2024b877e23",
   "metadata": {},
   "source": [
    "### 3.1.7. Analyse des résultats obtenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1381e7d-9f01-40a5-b061-3f8e8c907336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entraînement et évaluation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    y_pred = model.predict(X_val_vect)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85397ba7-febd-4169-bb86-55656d972fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dictionnaire pour stocker les métriques\n",
    "scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    y_pred = model.predict(X_val_vect)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    # Stockage des métriques\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    scores[name] = {\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1-score': report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6ace-646f-4a87-9111-6f17da4c2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les métriques\n",
    "labels = list(scores.keys())\n",
    "precision = [scores[model]['precision'] for model in labels]\n",
    "recall = [scores[model]['recall'] for model in labels]\n",
    "f1 = [scores[model]['f1-score'] for model in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "# Graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([p - width for p in x], precision, width=width, label='Précision')\n",
    "plt.bar(x, recall, width=width, label='Rappel')\n",
    "plt.bar([p + width for p in x], f1, width=width, label='F1-score')\n",
    "\n",
    "plt.xticks(x, labels, rotation=45)\n",
    "plt.ylabel(\"Score (macro avg)\")\n",
    "plt.title(\"Comparaison des métriques par modèle\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8168b5-92e4-4847-978c-41899e3fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-macos)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Encodage des labels (si ce n’est pas encore fait)\n",
    "encoder = LabelEncoder()\n",
    "y_train_enc = encoder.fit_transform(y_train)\n",
    "y_val_enc = encoder.transform(y_val)\n",
    "\n",
    "# Définition du modèle MLP\n",
    "model_mlp = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_vect.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(set(y_train_enc)), activation='softmax')  # Softmax pour multi-classe\n",
    "])\n",
    "\n",
    "model_mlp.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Entraînement\n",
    "model_mlp.fit(X_train_vect.toarray(), y_train_enc,\n",
    "              epochs=5, batch_size=32,\n",
    "              validation_data=(X_val_vect.toarray(), y_val_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88851e-a6fb-4771-abf4-67f1d3eecc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer le chemin vers Documents\n",
    "import os\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "documents_path = os.path.join(home, \"Documents\")\n",
    "\n",
    "# Sauvegarder les fichiers dans Documents\n",
    "train_df.to_csv(os.path.join(documents_path, \"train_df.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(documents_path, \"test_df.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(documents_path, \"val_df.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Fichiers enregistrés dans ton dossier Documents !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba48deb-1fc8-42e1-914e-bee5429361c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a050c39-6ed1-40d3-a7f8-3f9b1ea00603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
