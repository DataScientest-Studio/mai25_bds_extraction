{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a84f7f-71ef-49d6-9993-2e18d6f016c6",
   "metadata": {},
   "source": [
    "# Modèles multimodaux - CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abf6b4-8fec-440c-a7f1-33ac254319e2",
   "metadata": {},
   "source": [
    "## README\n",
    "Ce notebook permet la création et l'évaluation d'un modèle basé sur l'architecture existatne CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44804c6-78ae-482f-84c6-f6a741e8bd85",
   "metadata": {},
   "source": [
    "## 1. Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9a69b-bef6-4d16-bd29-ed9c8f074f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec9d71-5eca-472d-84fe-924c40100acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from src.models.multimodal_clip import MultiModalCLIPBasedClassifier\n",
    "from src.models.model_wrappers import ModelWrapperFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8697f72-68e1-4ac8-a304-049643520506",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb4c85-57b6-40af-ab93-02210642d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.read_parquet(PATHS.metadata / \"df_data_sets.parquet\")\n",
    "data_sets = pd.read_parquet(PATHS.metadata / \"df_data_sets.parquet\")\n",
    "labels = pd.read_parquet(PATHS.metadata / \"df_encoded_labels.parquet\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "266b98af-383d-40e9-99bb-c263794d2b8f",
   "metadata": {},
   "source": [
    "# pour ne travailler que sur un échantillon\n",
    "sample = pd.read_parquet(os.path.join(PATHS.metadata, 'samples', 'df_documents_sample_4k_2.parquet'))\n",
    "documents = sample.join(documents)\n",
    "labels = sample.join(labels)\n",
    "data_sets = sample.join(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f65ef1-1be3-48a2-97f3-2e182c26dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.shape, data_sets.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67ebb5-d49d-43ff-9c8e-808807053786",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = documents[data_sets.data_set == \"train\"].index\n",
    "y_train = labels[data_sets.data_set == \"train\"].label\n",
    "\n",
    "X_val = documents[data_sets.data_set == \"val\"].index\n",
    "y_val = labels[data_sets.data_set == \"val\"].label\n",
    "\n",
    "X_test = documents[data_sets.data_set == \"test\"].index\n",
    "y_test = labels[data_sets.data_set == \"test\"].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b52ee-7d8f-480d-a0b1-44c9296bf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalCLIPBasedClassifier()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d788ba5f-991d-408d-8fa0-f3062dc8c6a4",
   "metadata": {},
   "source": [
    "# CALCUL PREALABLE DES EMBEDDINGS\n",
    "# ETAPE TRES LONGUE LA PREMIERE FOIS (environ 8H00)\n",
    "# Une fois calculés, les embeddings seront sauvegardées et rechargés à chaque fois que requis.\n",
    "t0 = time.time()\n",
    "embeddings = model.clip.get_embeddings(documents.index)\n",
    "print(f\"Terminé en {time.time()-t0:.2f} secondes\")\n",
    "model.clip.save_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0de3b1-b50c-4c60-b55f-99ac32f12e89",
   "metadata": {},
   "source": [
    "# 3. Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02064da8-d27b-4f37-b77b-3722fb3e9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environ 3'30'' sur 400k images\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Terminé en {time.time()-t0:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b33715-7c6a-4fed-baf7-1f75953343a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATHS.models / \"mmo_clip_logistic_regressor.joblib\"\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dc46d-7aaf-4af9-8b3f-25eba67e0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"CLIP-based Logistic Regressor\"\n",
    "wrapper = ModelWrapperFactory.make_mmo_clip_wrapper(name, path)\n",
    "wrapper.visual_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04337cd5-b78e-45e5-82c6-09068b5f49c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
