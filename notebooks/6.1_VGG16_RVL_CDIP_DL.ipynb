{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c4868-c6d7-4fe2-aef8-243b8ab0f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import RandomTranslation, RandomZoom, RandomRotation\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS\n",
    "from src.visualization.visualize import draw_spider_graph_dark, conf_matrix_dark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703c459-c79a-408b-bf4f-2b191fab436d",
   "metadata": {},
   "source": [
    "## pour ne travailler que sur un échantillon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810e03b-841c-4016-b26d-b617c0b75746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_parquet(os.path.join(project_root,'data', 'metadata', 'samples', 'df_documents_sample_40k_1.parquet'), engine='fastparquet')\n",
    "\n",
    "converted_prefix = os.path.join(project_root, 'data', 'converted')\n",
    "sample['filepath'] = sample['rvl_image_path'].apply(lambda p: os.path.join(converted_prefix, p.replace(\"raw/\", \"\").replace(\".tif\", \".jpg\")))\n",
    "sample = sample.drop(columns=['rvl_image_path', 'document_id', 'filename', 'iit_image_path', 'iit_individual_xml_path', 'iit_collective_xml_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e71a0-27db-4e0a-a393-8697a0cbe664",
   "metadata": {},
   "source": [
    "## création des sets train, test et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33ce64-a403-413d-b212-ebbf0701f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encodage des labels : pas besoin ? \n",
    "label_encoder = LabelEncoder()\n",
    "sample['label_encoded'] = label_encoder.fit_transform(sample['label'])\n",
    "\n",
    "# 2. On part de sample pour créer les différents sets\n",
    "df_train = sample[sample['data_set'] == 'train']\n",
    "df_val = sample[sample['data_set'] == 'val']\n",
    "df_test = sample[sample['data_set'] == 'test']\n",
    "\n",
    "# 3. Fonction pour charger et prétraiter une image\n",
    "def process_image(file_path, label):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) #parce que VGG16 attend 3 canaux\n",
    "    image = tf.image.resize(image, [224, 224])  # taille attendue par VGG16\n",
    "    image = image / 255.0  # Normalisation entre 0 et 1\n",
    "    return image, label\n",
    "\n",
    "# 4. Création du dataset\n",
    "def get_dataset(df_subset, shuffle=False):\n",
    "    file_paths = df_subset['filepath'].values\n",
    "    labels = df_subset['label_encoded'].values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: process_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_dataset(df_train, shuffle=True)\n",
    "val_ds = get_dataset(df_val)\n",
    "test_ds = get_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7fbbb-4c0e-4d70-b594-dfb4ce8c5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds) #attention c'est le nombre de batchs de 32 images, et il y en a 1 pas tout à fait rempli. Ce nombre est donc attendu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d68f54-6ec6-47d4-a660-e6d3a0c8557b",
   "metadata": {},
   "source": [
    "## On récupère le modèle VGG16 pour le réentrainer sur nos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a1c1d-8d6a-4f52-8946-eac2c38bd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle VGG16\n",
    "base_model = keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freezer les couches du VGG16\n",
    "base_model.trainable = False\n",
    "\n",
    "# Redégeler les couches après chargement\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Création du modèle avec l'API Fonctionnelle\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Application des augmentations                          \n",
    "#x = RandomTranslation(height_factor=0.1, width_factor=0.1)(inputs) \n",
    "#x = RandomZoom(0.1)(x)  \n",
    "\n",
    "# Construction du modèle\n",
    "x = base_model(inputs)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "outputs = Dense(16, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d78c45-7fdb-4102-b38a-eaf33172afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b218957-a7a3-4116-ab80-5105474e26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "                                patience=5, # Attendre 5 epochs avant application\n",
    "                                min_delta=0.001, # si au bout de 5 epochs la fonction de perte ne varie pas de 1%, \n",
    "    # que ce soit à la hausse ou à la baisse, on arrête\n",
    "                                verbose=1, # Afficher à quel epoch on s'arrête\n",
    "                                mode='min',\n",
    "                                monitor='val_loss')\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                        patience=5,        # attendre un peu plus\n",
    "                                        min_delta=0.001,   # plus sensible\n",
    "                                        factor=0.5,        # réduire plus doucement\n",
    "                                        cooldown=3,\n",
    "                                        min_lr=1e-6,\n",
    "                                        verbose=1\n",
    "                                    )\n",
    "\n",
    "checkpoint = ModelCheckpoint( #retient le meilleur modèle\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db53fd-0236-4f80-8497-2b6b27086fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model = model.fit(train_ds, \n",
    "                          epochs=50,\n",
    "                          validation_data=val_ds, \n",
    "                          callbacks = [reduce_learning_rate,\n",
    "                                       early_stopping, \n",
    "                                       checkpoint])\n",
    "# je ne touche pas au set de validation pour l'instant, seulement, quand j'aurai des résultats satisfaisants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34d745-fc73-4034-9958-1ba238bb4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model.save(os.path.join(project_root, 'models', 'VGG16_best_50_epocs_sample_40_000.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab4ba1-0d74-4828-bb49-25df97d09dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history_model.history['loss'])\n",
    "plt.plot(history_model.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history_model.history['accuracy'])\n",
    "plt.plot(history_model.history['val_accuracy'])\n",
    "plt.title('Model accuracy by epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc687a53-bf11-4392-8c7f-7641e9981507",
   "metadata": {},
   "source": [
    "## On evalue la qualité de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038e7c9-1d9e-4e73-a17d-a29639ff311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.saving.load_model(os.path.join(project_root, 'models', 'VGG16_best_50_epocs_sample_40_000.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990fb38-b2ba-433d-87be-e1d3f5cca3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Prédire sur le test set\n",
    "y_pred_probs = loaded_model.predict(test_ds)  # Probabilités\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Classes prédites\n",
    "\n",
    "# Étape 2 : Récupérer les vrais labels depuis le test set, dans le même ordre\n",
    "y_true_check = []\n",
    "for batch in test_ds:\n",
    "    images, labels = batch\n",
    "    y_true_check.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true_check)\n",
    "\n",
    "# Étape 3 : Rapport de classification\n",
    "print(\"\\n Rapport de classification :\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8ebb4-e157-480a-a942-5a81faa35c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Matrice de confusion\n",
    "print(\"Matrice de confusion :\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661c27b-8081-4aec-ae4e-8aee239e65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_dark(cm, \"illustrations/vgg16_1_cm.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b4e51-645c-4f0f-bd02-e4950a7ff3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_spider_graph_dark(y_true, y_pred, save_path=\"illustrations/vgg16_1_spider.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6ffb6-fded-4a03-bab7-beb8cbadf0f1",
   "metadata": {},
   "source": [
    "# Maintenant, on essaie d'encore améliorer ça en faisant un progressive unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d092b59-19fb-4656-af74-2aa8730b7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942e588-d692-4b34-99e1-98ba8f111ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de base VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# (Optionnel) dégeler un peu au début\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Définir l’architecture complète\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(16, activation='softmax')(x)  # 16 classes\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# Compiler\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73f0de-3829-4eb8-baa4-875917d842a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les callbacks\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class SafeUnfreezeCallback(Callback):\n",
    "    def __init__(self, base_model, model_path='best_model.keras',\n",
    "                 unfreeze_step=5, max_unfreeze=30,\n",
    "                 patience=5, min_delta=0.001):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.model_path = model_path\n",
    "        self.unfreeze_step = unfreeze_step\n",
    "        self.max_unfreeze = max_unfreeze\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.current_unfrozen = 4\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "        self.trigger_reload = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is None:\n",
    "            return\n",
    "\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "\n",
    "        if self.wait >= self.patience and self.current_unfrozen < self.max_unfreeze:\n",
    "            total_layers = len(self.base_model.layers)\n",
    "            start = max(total_layers - self.current_unfrozen - self.unfreeze_step, 0)\n",
    "            end = total_layers - self.current_unfrozen\n",
    "            unfrozen = 0\n",
    "            for layer in self.base_model.layers[start:end]:\n",
    "                # Important : ne pas dégeler les BatchNormalization\n",
    "                if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                    layer.trainable = True\n",
    "                    unfrozen += 1\n",
    "\n",
    "            self.current_unfrozen += unfrozen\n",
    "            print(f\"\\n🔓 Dégel de {unfrozen} couches supplémentaires (total dégelées : {self.current_unfrozen})\")\n",
    "\n",
    "            # Réduction du learning rate\n",
    "            old_lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
    "            new_lr = max(old_lr * 0.5, 1e-5)\n",
    "            try:\n",
    "                K.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "            except AttributeError:\n",
    "                print(\"⚠️ Impossible de modifier le learning rate — mauvais type. Recréation de l'optimiseur avec le nouveau LR.\")\n",
    "                self.model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=new_lr),\n",
    "                    loss=self.model.loss,\n",
    "                    metrics=self.model.metrics,\n",
    "                )\n",
    "            print(f\"📉 Nouveau learning rate : {old_lr:.2e} → {new_lr:.2e}\")\n",
    "\n",
    "            # Stop pour recharger le meilleur modèle\n",
    "            print(\"⚠️ Entraînement interrompu → rechargement du meilleur modèle\")\n",
    "            self.model.stop_training = True\n",
    "            self.trigger_reload = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.wait = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.trigger_reload = False\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "                                patience=5, # Attendre 5 epochs avant application\n",
    "                                min_delta=0.001, # si au bout de 5 epochs la fonction de perte ne varie pas de 1%, \n",
    "    # que ce soit à la hausse ou à la baisse, on arrête\n",
    "                                verbose=1, # Afficher à quel epoch on s'arrête\n",
    "                                mode='min',\n",
    "                                monitor='val_loss')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                        patience=4,        \n",
    "                                        min_delta=0.001,   # plus sensible\n",
    "                                        factor=0.5,        # réduire plus doucement\n",
    "                                        cooldown=3,\n",
    "                                        min_lr=1e-6,\n",
    "                                        verbose=1\n",
    "                                    )\n",
    "\n",
    "checkpoint = ModelCheckpoint( #retient le meilleur modèle\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "unfreeze_cb = SafeUnfreezeCallback(base_model=base_model,\n",
    "                                   unfreeze_step=5,\n",
    "                                   max_unfreeze=100,\n",
    "                                   patience=5,\n",
    "                                   min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3142e-4672-47bb-9dbb-e0455336c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = 10  # Nombre maximal de phases d'entraînement\n",
    "combined_history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "\n",
    "for round_idx in range(max_rounds):\n",
    "    print(f\"\\n🔁 Phase d'entraînement {round_idx + 1}\")\n",
    "    \n",
    "    # 🔄 Réinitialise proprement le callback\n",
    "    unfreeze_cb.reset()\n",
    "\n",
    "    # Recharger le meilleur modèle\n",
    "    model.load_weights('best_model.keras')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                      learning_rate=float(K.get_value(model.optimizer.learning_rate))),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=15,\n",
    "                        callbacks=[unfreeze_cb, early_stopping, reduce_lr, checkpoint])\n",
    "\n",
    "    # Combiner les historiques\n",
    "    for key in combined_history:\n",
    "        combined_history[key] += history.history.get(key, [])\n",
    "\n",
    "    if not unfreeze_cb.trigger_reload:\n",
    "        print(\"\\n✅ Entraînement terminé — plus de couches à dégeler ou amélioration suffisante.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66fda9-5612-4ed7-bdc7-e031b603ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(combined_history['loss'])\n",
    "plt.plot(combined_history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(combined_history['accuracy'])\n",
    "plt.plot(combined_history['val_accuracy'])\n",
    "plt.title('Model accuracy by epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfe19d-4b8f-4539-a8e3-4c3195d4ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the failed model anyway\n",
    "model.save(os.path.join(project_root, 'models', 'VGG16_best_failed_degel.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c10fb1-459e-408d-8ecf-41214b87b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.saving.load_model(os.path.join(project_root, 'models', 'VGG16_best_progressive_unfreeze_40_000.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423edf3-1d01-4868-b93f-6a84ae63ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Prédire sur le test set\n",
    "y_pred_probs = loaded_model.predict(test_ds)  # Probabilités\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Classes prédites\n",
    "\n",
    "# Étape 2 : Récupérer les vrais labels depuis le test set, dans le même ordre\n",
    "y_true_check = []\n",
    "for batch in test_ds:\n",
    "    images, labels = batch\n",
    "    y_true_check.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true_check)\n",
    "\n",
    "# Étape 3 : Rapport de classification\n",
    "print(\"\\n Rapport de classification :\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923e405-ef99-4bd4-ad31-86875c8f219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Matrice de confusion\n",
    "print(\"Matrice de confusion :\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee317c8d-b3a9-44dd-8687-0c34f9f8adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_dark(cm, \"illustrations/vgg16_2_cm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba7c51-eee4-4dfb-bfa4-10396f40ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_spider_graph_dark(y_true, y_pred, save_path=\"illustrations/vgg16_2_spider.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b77b-ef8c-48ed-a045-272434bf8043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a8c1e-8135-4363-bd1d-45d3acf43b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493349-7214-4b0e-a8bd-948630428ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efed44-1576-40b7-9dc3-ca4b6c1c6005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
