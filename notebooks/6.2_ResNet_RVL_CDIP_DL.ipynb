{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d63304-e303-40ec-b4f1-7aea32395827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import RandomTranslation, RandomZoom, RandomRotation\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if not project_root in [Path(p).resolve() for p in sys.path]:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import PATHS\n",
    "from src.visualization.visualize import draw_spider_graph_dark, conf_matrix_dark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa87155-e2c4-472c-abd1-15caf4213adf",
   "metadata": {},
   "source": [
    "## Travail sur un échantilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e8274-80ce-408b-832f-1087ae3b2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_parquet(os.path.join(project_root,'data', 'metadata', 'samples', 'df_documents_sample_40k_1.parquet'), engine='fastparquet')\n",
    "\n",
    "converted_prefix = os.path.join(project_root, 'data', 'converted')\n",
    "sample['filepath'] = sample['rvl_image_path'].apply(lambda p: os.path.join(converted_prefix, p.replace(\"raw/\", \"\").replace(\".tif\", \".jpg\")))\n",
    "sample = sample.drop(columns=['rvl_image_path', 'document_id', 'filename', 'iit_image_path', 'iit_individual_xml_path', 'iit_collective_xml_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c77113-5673-435f-ab80-db34e586c358",
   "metadata": {},
   "source": [
    "## Création des sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67304c64-25ed-4c20-8875-f7410b186155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encodage des labels : pas besoin ? \n",
    "label_encoder = LabelEncoder()\n",
    "sample['label_encoded'] = label_encoder.fit_transform(sample['label'])\n",
    "\n",
    "# 2. On part de sample pour créer les différents sets\n",
    "df_train = sample[sample['data_set'] == 'train']\n",
    "df_val = sample[sample['data_set'] == 'val']\n",
    "df_test = sample[sample['data_set'] == 'test']\n",
    "\n",
    "# 3. Fonction pour charger et prétraiter une image\n",
    "def process_image(file_path, label, augment=False):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = preprocess_input(image)\n",
    "    if augment:\n",
    "        image = data_augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "# 4. Création du dataset\n",
    "def get_dataset(df_subset, shuffle=False, augment=False):\n",
    "    file_paths = df_subset['filepath'].values\n",
    "    labels = df_subset['label_encoded'].values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: process_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_dataset(df_train, shuffle=True)\n",
    "val_ds = get_dataset(df_val)\n",
    "test_ds = get_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621ac22-97a0-4562-ae09-58b42b275677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Charger la base ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Geler les couches sauf 10\n",
    "base_model.trainable = False\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Construction du modèle\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "#On teste quelques augmentations: NON, ça donne des résultats mauvais\n",
    "#x = tf.keras.layers.RandomRotation(0.02)(inputs)\n",
    "#x = tf.keras.layers.RandomZoom(0.1)(x)\n",
    "#x = tf.keras.layers.RandomContrast(0.1)(x)\n",
    "#x = tf.keras.layers.RandomTranslation(0.05, 0.05)(x)\n",
    "\n",
    "#on envoie tout ça dans le modèle\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(16, activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001, verbose=1, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_resnet_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Entraînement\n",
    "history_1 = model.fit(train_ds,\n",
    "                      validation_data=val_ds,\n",
    "                      epochs=10,\n",
    "                      callbacks=[early_stopping, reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf0655-895d-4e51-b537-93dca6fa0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On relance après avoir dégelé des couches (il y en a 177 en tout dans ResNet donc là on en dégèle 27 supplémentaires, soit 37\n",
    "\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:140]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_2 = model.fit(train_ds,\n",
    "                      validation_data=val_ds,\n",
    "                      epochs=10,\n",
    "                      callbacks=[early_stopping, reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb53f65-b915-48f3-bd0f-6b5e9d8014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encore une étape de dégelage, on rajoute 20 couches à entrainer \n",
    "\n",
    "for layer in base_model.layers[:120]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model.fit(train_ds,\n",
    "                      validation_data=val_ds,\n",
    "                      epochs=10,\n",
    "                      callbacks=[early_stopping, reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8796c2-feb3-4683-a495-0443effb6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_histories(*histories):\n",
    "    combined = {}\n",
    "    for key in histories[0].history.keys():\n",
    "        combined[key] = sum((h.history[key] for h in histories), [])\n",
    "    return combined\n",
    "\n",
    "combined_history = combine_histories(history_1, history_2, history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f627a0e-7328-47fa-a32c-6cc0c7bb4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model.save(os.path.join(project_root,'models','ResNet50_best_30_epocs_sample_40_000_unfreeze_step_by_step.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa735e-48c2-4879-8a7c-9dd322aa7b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92efcd-1a67-4981-b0eb-23c19ed1823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(combined_history['loss'])\n",
    "plt.plot(combined_history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(combined_history['accuracy'])\n",
    "plt.plot(combined_history['val_accuracy'])\n",
    "plt.title('Model accuracy by epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710a146-aab4-402e-b239-78a1ec6a18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.saving.load_model(os.path.join(project_root,'models','ResNet50_best_30_epocs_sample_40_000_unfreeze_step_by_step.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabcd66-7c20-4248-a0e9-5e75f9e03f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Étape 1 : Prédire sur le test set\n",
    "y_pred_probs = model.predict(test_ds)  # Probabilités\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Classes prédites\n",
    "\n",
    "# Étape 2 : Récupérer les vrais labels depuis le test set, dans le même ordre\n",
    "y_true_check = []\n",
    "for batch in test_ds:\n",
    "    images, labels = batch\n",
    "    y_true_check.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true_check)\n",
    "\n",
    "# Étape 3 : Rapport de classification\n",
    "print(\"\\n Rapport de classification :\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80058a-1d31-4c70-8753-61f9d6da4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix_dark(cm, \"illustrations/ResNet_2_cm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57351bf9-e84c-4dc4-a966-195e02bbcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_spider_graph_dark(y_true, y_pred, save_path=\"illustrations/ResNet_2_spider.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e06271-7126-4149-985f-6214e82ed59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Matrice de confusion\n",
    "print(\"Matrice de confusion :\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fccd37-3f46-43fb-882b-38ce1106395e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86d13b-9e25-4332-b2ae-81ec08e882b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83bbc7f8-b726-4ff7-9e0c-90783436a00e",
   "metadata": {},
   "source": [
    "# degel progressif automatique: FAILED\n",
    "Aucune idée pourquoi mais, dès le début, ça n'apprend pas, la loss ne fait qu'augmenter, et redescend par accoups au moment des dégels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec416bef-80fa-4342-a926-3ae7e62c6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Charger la base ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Geler les couches sauf 10\n",
    "base_model.trainable = False\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Définir l’architecture complète\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(16, activation='softmax')(x)  # 16 classes\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compilation initiale\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9398c-6ca9-43f4-b22a-b2de103091a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class SafeUnfreezeCallback(Callback):\n",
    "    def __init__(self, base_model, model_path='best_model.keras',\n",
    "                 unfreeze_step=5, max_unfreeze=30,\n",
    "                 patience=5, min_delta=0.001):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.model_path = model_path\n",
    "        self.unfreeze_step = unfreeze_step\n",
    "        self.max_unfreeze = max_unfreeze\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.current_unfrozen = 10\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "        self.trigger_reload = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is None:\n",
    "            return\n",
    "\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "\n",
    "        if self.wait >= self.patience and self.current_unfrozen < self.max_unfreeze:\n",
    "            total_layers = len(self.base_model.layers)\n",
    "            start = max(total_layers - self.current_unfrozen - self.unfreeze_step, 0)\n",
    "            end = total_layers - self.current_unfrozen\n",
    "            unfrozen = 0\n",
    "            for layer in self.base_model.layers[start:end]:\n",
    "                # Important : ne pas dégeler les BatchNormalization\n",
    "                if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                    layer.trainable = True\n",
    "                    unfrozen += 1\n",
    "\n",
    "            self.current_unfrozen += unfrozen\n",
    "            print(f\"\\n🔓 Dégel de {unfrozen} couches supplémentaires (total dégelées : {self.current_unfrozen})\")\n",
    "\n",
    "            # Réduction du learning rate\n",
    "            old_lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
    "            new_lr = max(old_lr * 0.5, 1e-5)\n",
    "            try:\n",
    "                K.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "            except AttributeError:\n",
    "                print(\"⚠️ Impossible de modifier le learning rate — mauvais type. Recréation de l'optimiseur avec le nouveau LR.\")\n",
    "                self.model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=new_lr),\n",
    "                    loss=self.model.loss,\n",
    "                    metrics=self.model.metrics,\n",
    "                )\n",
    "            print(f\"📉 Nouveau learning rate : {old_lr:.2e} → {new_lr:.2e}\")\n",
    "\n",
    "            # Stop pour recharger le meilleur modèle\n",
    "            print(\"⚠️ Entraînement interrompu → rechargement du meilleur modèle\")\n",
    "            self.model.stop_training = True\n",
    "            self.trigger_reload = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.wait = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.trigger_reload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c21e9e6-25a4-4a99-bd25-1ed75277d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001, monitor='val_loss', mode='min', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_resnet_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Initialisation de notre callback personnalisé\n",
    "unfreeze_cb = SafeUnfreezeCallback(base_model=base_model,\n",
    "                                   unfreeze_step=5,\n",
    "                                   max_unfreeze=100,\n",
    "                                   patience=5,\n",
    "                                   min_delta=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfcd5d-05b6-48ba-b793-a1faae14d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = 10  # Nombre maximal de phases d'entraînement\n",
    "combined_history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "\n",
    "for round_idx in range(max_rounds):\n",
    "    print(f\"\\n🔁 Phase d'entraînement {round_idx + 1}\")\n",
    "    \n",
    "    # 🔄 Réinitialise proprement le callback\n",
    "    unfreeze_cb.reset()\n",
    "\n",
    "    # Recharger le meilleur modèle\n",
    "    model.load_weights('best_resnet_model.keras')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                      learning_rate=float(K.get_value(model.optimizer.learning_rate))),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=15,\n",
    "                        callbacks=[unfreeze_cb, early_stopping, reduce_lr, checkpoint])\n",
    "\n",
    "    # Combiner les historiques\n",
    "    for key in combined_history:\n",
    "        combined_history[key] += history.history.get(key, [])\n",
    "\n",
    "    if not unfreeze_cb.trigger_reload:\n",
    "        print(\"\\n✅ Entraînement terminé — plus de couches à dégeler ou amélioration suffisante.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72436826-cada-4c98-a9da-36bfaf8eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(combined_history['loss'])\n",
    "plt.plot(combined_history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(combined_history['accuracy'])\n",
    "plt.plot(combined_history['val_accuracy'])\n",
    "plt.title('Model accuracy by epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf61d48-b529-4741-bde2-cbdede1148ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
